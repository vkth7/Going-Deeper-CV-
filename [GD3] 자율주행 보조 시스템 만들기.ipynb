{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkth7/Going-Deeper-CV-/blob/main/%5BGD3%5D%20%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89%20%EB%B3%B4%EC%A1%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc344bc9",
      "metadata": {
        "id": "cc344bc9"
      },
      "source": [
        "# 프로젝트: 자율주행 보조 시스템 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57abf38a",
      "metadata": {
        "id": "57abf38a"
      },
      "outputs": [],
      "source": [
        "import os, copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "DATA_PATH = os.getenv('HOME') + '/aiffel/object_detection/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6f209c",
      "metadata": {
        "id": "3c6f209c"
      },
      "outputs": [],
      "source": [
        "# KITTI dataset 다운\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'kitti',\n",
        "    data_dir=DATA_PATH,\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a38a6d9",
      "metadata": {
        "id": "9a38a6d9"
      },
      "outputs": [],
      "source": [
        "def swap_xy(boxes):\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b158001",
      "metadata": {
        "id": "4b158001"
      },
      "outputs": [],
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        boxes = tf.stack(\n",
        "           [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "        \n",
        "    return image, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98ecd5e",
      "metadata": {
        "id": "b98ecd5e"
      },
      "outputs": [],
      "source": [
        "def resize_and_pad_image(image, training=True):\n",
        "\n",
        "    min_side = 800.0\n",
        "    max_side = 1333.0\n",
        "    min_side_range = [640, 1024]\n",
        "    stride = 128.0\n",
        "    \n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if training:\n",
        "        min_side = tf.random.uniform((), min_side_range[0], min_side_range[1], dtype=tf.float32)\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a686e1",
      "metadata": {
        "id": "b0a686e1"
      },
      "outputs": [],
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2f947d",
      "metadata": {
        "id": "8c2f947d"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(sample):\n",
        "    image = sample[\"image\"]\n",
        "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    class_id = tf.cast(sample[\"objects\"][\"type\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c95c12b",
      "metadata": {
        "id": "7c95c12b"
      },
      "outputs": [],
      "source": [
        "class AnchorBox:\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11e4f4f",
      "metadata": {
        "id": "c11e4f4f"
      },
      "outputs": [],
      "source": [
        "def convert_to_corners(boxes):\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a340e2",
      "metadata": {
        "id": "b7a340e2"
      },
      "outputs": [],
      "source": [
        "class LabelEncoder:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ace46c",
      "metadata": {
        "id": "c8ace46c"
      },
      "outputs": [],
      "source": [
        "class FeaturePyramid(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, backbone):\n",
        "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\")\n",
        "        self.backbone = backbone\n",
        "        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1a5d39",
      "metadata": {
        "id": "de1a5d39"
      },
      "outputs": [],
      "source": [
        "def build_head(output_filters, bias_init):\n",
        "    head = tf.keras.Sequential([tf.keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            tf.keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
        "        )\n",
        "        head.add(tf.keras.layers.ReLU())\n",
        "    head.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005c6993",
      "metadata": {
        "id": "005c6993"
      },
      "outputs": [],
      "source": [
        "def get_backbone():\n",
        "    backbone = tf.keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return tf.keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f8eca9",
      "metadata": {
        "id": "52f8eca9"
      },
      "outputs": [],
      "source": [
        "class RetinaNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes, backbone):\n",
        "        super(RetinaNet, self).__init__(name=\"RetinaNet\")\n",
        "        self.fpn = FeaturePyramid(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0115c81",
      "metadata": {
        "id": "c0115c81"
      },
      "outputs": [],
      "source": [
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(RetinaNetBoxLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super(RetinaNetClassificationLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "\n",
        "    def __init__(self, num_classes=8, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f347c19",
      "metadata": {
        "id": "0f347c19"
      },
      "outputs": [],
      "source": [
        "num_classes = 8\n",
        "batch_size = 2\n",
        "\n",
        "resnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "model = RetinaNet(num_classes, resnet50_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c70e9f9",
      "metadata": {
        "id": "3c70e9f9"
      },
      "outputs": [],
      "source": [
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7bf7b9",
      "metadata": {
        "id": "ab7bf7b9",
        "outputId": "c275c390-8933-4adf-a50b-d572cb1977fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
        "    \"kitti\", split=[\"train\", \"validation\"], with_info=True, data_dir=DATA_PATH\n",
        ")\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
        "train_dataset = train_dataset.padded_batch(\n",
        "    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "train_dataset = train_dataset.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "train_dataset = train_dataset.prefetch(autotune)\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.prefetch(autotune)\n",
        "\n",
        "print('슝=3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a949f79",
      "metadata": {
        "id": "5a949f79",
        "outputId": "51b45392-dbf4-4215-818e-c1012437cd04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 462/3173 [===>..........................] - ETA: 22:09 - loss: 1.2993"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_687/3802483343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_dir = os.getenv('HOME') + '/aiffel/object_detection/data/checkpoints2/'\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks_list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034433da",
      "metadata": {
        "id": "034433da"
      },
      "outputs": [],
      "source": [
        "model_dir = os.getenv('HOME') + '/aiffel/object_detection/data/checkpoints/'\n",
        "latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "model.load_weights(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99685230",
      "metadata": {
        "id": "99685230"
      },
      "outputs": [],
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=8,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2]\n",
        "    ):\n",
        "        super(DecodePredictions, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            box_variance, dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )\n",
        "\n",
        "print('슝=3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad23fec2",
      "metadata": {
        "id": "ad23fec2"
      },
      "outputs": [],
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a25ffb",
      "metadata": {
        "id": "91a25ffb"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image, training=False)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b8e5ab",
      "metadata": {
        "id": "76b8e5ab"
      },
      "outputs": [],
      "source": [
        "def box_size_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    \n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        \n",
        "    return w, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5b6159",
      "metadata": {
        "id": "ee5b6159"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        origin_x, origin_y = x1, image.shape[0] - y2 # matplitlib에서 Rectangle와 text를 그릴 때는 좌하단이 원점이고 위로 갈 수록 y값이 커집니다\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [origin_x, origin_y], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            origin_x,\n",
        "            origin_y,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f975264a",
      "metadata": {
        "id": "f975264a"
      },
      "source": [
        "## 1. 자율주행 시스템 만들기\n",
        "---\n",
        "\n",
        "위에서 만든 모델을 통해 아래의 조건을 만족하는 함수를 만들어 주세요.\n",
        "\n",
        "- 입력으로 이미지 경로를 받습니다.\n",
        "- 정지조건에 맞는 경우 \"Stop\" 아닌 경우 \"Go\"를 반환합니다.\n",
        "- 조건은 다음과 같습니다.\n",
        "    \n",
        "    <br>\n",
        "    \n",
        "    - 사람이 한 명 이상 있는 경우\n",
        "    - 차량의 크기(width or height)가 300px이상인 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c3fdd6",
      "metadata": {
        "id": "09c3fdd6"
      },
      "outputs": [],
      "source": [
        "img_path = os.getenv('HOME')+'/aiffel/object_detection/data/stop_1.png'\n",
        "int2str = dataset_info.features[\"objects\"][\"type\"].int2str\n",
        "# OpenCV로 이미지를 불러오기\n",
        "img = Image.open(img_path)\n",
        "img = tf.cast(img, dtype=tf.float32)\n",
        "input_img, ratio = prepare_image(img)\n",
        "detections = inference_model.predict(input_img)\n",
        "num_detections = detections.valid_detections[0]\n",
        "class_names = [\n",
        "    int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "]\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15dd325",
      "metadata": {
        "id": "f15dd325"
      },
      "outputs": [],
      "source": [
        "img_path = os.getenv('HOME')+'/aiffel/object_detection/data/go_2.png'\n",
        "int2str = dataset_info.features[\"objects\"][\"type\"].int2str\n",
        "def self_drive_assist(img_path, size_limit=300):\n",
        "    # 코드 구현\n",
        "        # 정지조건에 맞으면 return \"Stop\"\n",
        "        # 아닌 경우 return \"Go\"\n",
        "        \n",
        "    img = Image.open(img_path)\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    input_img, ratio = prepare_image(img)\n",
        "    \n",
        "    detections = inference_model.predict(input_img)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [\n",
        "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "    ]\n",
        "    w, h = box_size_detections(\n",
        "        img,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )\n",
        "    \n",
        "    visualize_detections(\n",
        "        img,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )\n",
        "    if 'Pedestrian' in class_names:        \n",
        "        return 'Stop'\n",
        "    elif w >= 300 or h >= 300:\n",
        "        return 'Stop'\n",
        "    else:\n",
        "        return 'Go'\n",
        "\n",
        "print(self_drive_assist(img_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6654476",
      "metadata": {
        "id": "f6654476"
      },
      "source": [
        "## 2. 자율주행 시스템 평가하기\n",
        "---\n",
        "\n",
        "아래 ```test_system()```를 통해서 위에서 만든 함수를 평가해봅시다. 10장에 대해 Go와 Stop을 맞게 반환하는지 확인하고 100점 만점으로 평가해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbbbb30",
      "metadata": {
        "id": "fcbbbb30"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def test_system(func):\n",
        "    work_dir = os.getenv('HOME')+'/aiffel/object_detection/data'\n",
        "    score = 0\n",
        "    test_set=[\n",
        "        (\"stop_1.png\", \"Stop\"),\n",
        "        (\"stop_2.png\", \"Stop\"),\n",
        "        (\"stop_3.png\", \"Stop\"),\n",
        "        (\"stop_4.png\", \"Stop\"),\n",
        "        (\"stop_5.png\", \"Stop\"),\n",
        "        (\"go_1.png\", \"Go\"),\n",
        "        (\"go_2.png\", \"Go\"),\n",
        "        (\"go_3.png\", \"Go\"),\n",
        "        (\"go_4.png\", \"Go\"),\n",
        "        (\"go_5.png\", \"Go\"),\n",
        "    ]\n",
        "    \n",
        "    for image_file, answer in test_set:\n",
        "        image_path = work_dir + '/' + image_file\n",
        "        pred = func(image_path)\n",
        "        if pred == answer:\n",
        "            score += 10\n",
        "        else:\n",
        "            print('pred: ', pred)\n",
        "            print('answer: ', answer)\n",
        "    print(f\"{score}점입니다.\")\n",
        "\n",
        "test_system(self_drive_assist)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}